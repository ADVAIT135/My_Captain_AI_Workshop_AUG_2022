{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport sys\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\nimport nltk\nnltk.download('stopwords')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-21T12:30:09.844389Z","iopub.execute_input":"2022-08-21T12:30:09.845165Z","iopub.status.idle":"2022-08-21T12:30:09.964786Z","shell.execute_reply.started":"2022-08-21T12:30:09.845129Z","shell.execute_reply":"2022-08-21T12:30:09.963782Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"file = open('../input/frankenstein-2/frankenstein-2.txt').read()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:30:18.182659Z","iopub.execute_input":"2022-08-21T12:30:18.183306Z","iopub.status.idle":"2022-08-21T12:30:18.189498Z","shell.execute_reply.started":"2022-08-21T12:30:18.183270Z","shell.execute_reply":"2022-08-21T12:30:18.188313Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# Tokenize words from the data\n# Standardization\ndef tokenize_words(input):\n  input = input.lower()\n  tokenizer = RegexpTokenizer(r'\\w+')\n  tokens = tokenizer.tokenize(input)\n  filtered = filter(lambda token : token not in stopwords.words('english'),tokens)\n  return \"\".join(filtered)\nprocessed_inputs = tokenize_words(file)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:30:20.302929Z","iopub.execute_input":"2022-08-21T12:30:20.304123Z","iopub.status.idle":"2022-08-21T12:30:29.613400Z","shell.execute_reply.started":"2022-08-21T12:30:20.304072Z","shell.execute_reply":"2022-08-21T12:30:29.612415Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# chars to numbers\nchars = sorted(list(set(processed_inputs)))\nchar_to_nums = dict((c,i) for i,c in enumerate(chars))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:30:34.562491Z","iopub.execute_input":"2022-08-21T12:30:34.562870Z","iopub.status.idle":"2022-08-21T12:30:34.571515Z","shell.execute_reply.started":"2022-08-21T12:30:34.562838Z","shell.execute_reply":"2022-08-21T12:30:34.570243Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# To verify whether words to char or char_to_num has worked\ninput_len = len(processed_inputs)\nvocab_len = len(chars)\nprint(\"Total number of characters : \", input_len)\nprint(\"Total Vocab : \", vocab_len)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:30:42.723086Z","iopub.execute_input":"2022-08-21T12:30:42.723716Z","iopub.status.idle":"2022-08-21T12:30:42.729716Z","shell.execute_reply.started":"2022-08-21T12:30:42.723681Z","shell.execute_reply":"2022-08-21T12:30:42.728400Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Total number of characters :  220931\nTotal Vocab :  37\n","output_type":"stream"}]},{"cell_type":"code","source":"# seq length\nseq_lenght = 100\nx_data = []\ny_data = []","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:30:53.844444Z","iopub.execute_input":"2022-08-21T12:30:53.844938Z","iopub.status.idle":"2022-08-21T12:30:53.933590Z","shell.execute_reply.started":"2022-08-21T12:30:53.844891Z","shell.execute_reply":"2022-08-21T12:30:53.932523Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# loop through the sequence\nfor i in range(0, input_len-seq_lenght, 1):\n  in_seq = processed_inputs[i:i + seq_lenght]\n  out_seq = processed_inputs[i + seq_lenght]\n  x_data.append([char_to_nums[char] for char in in_seq])\n  y_data.append(char_to_nums[out_seq])\nn_patterns = len(x_data)\nprint(\"Total Patterns : \", n_patterns)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:30:58.592031Z","iopub.execute_input":"2022-08-21T12:30:58.593122Z","iopub.status.idle":"2022-08-21T12:31:01.505135Z","shell.execute_reply.started":"2022-08-21T12:30:58.593078Z","shell.execute_reply":"2022-08-21T12:31:01.504004Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Total Patterns :  220831\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert input_sequence to np array and so on\nx = np.reshape(x_data,(n_patterns,seq_lenght,1))\nx = x/float(vocab_len)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:31:03.702561Z","iopub.execute_input":"2022-08-21T12:31:03.702938Z","iopub.status.idle":"2022-08-21T12:31:05.360349Z","shell.execute_reply.started":"2022-08-21T12:31:03.702904Z","shell.execute_reply":"2022-08-21T12:31:05.359241Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding\ny = np_utils.to_categorical(y_data)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:31:42.063635Z","iopub.execute_input":"2022-08-21T12:31:42.064033Z","iopub.status.idle":"2022-08-21T12:31:42.087782Z","shell.execute_reply.started":"2022-08-21T12:31:42.063998Z","shell.execute_reply":"2022-08-21T12:31:42.086878Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Creating the model \nmodel  = Sequential()\nmodel.add(LSTM(256, input_shape = (x.shape[1], x.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, return_sequences= True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation = 'softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:31:47.507307Z","iopub.execute_input":"2022-08-21T12:31:47.507664Z","iopub.status.idle":"2022-08-21T12:31:48.137131Z","shell.execute_reply.started":"2022-08-21T12:31:47.507634Z","shell.execute_reply":"2022-08-21T12:31:48.136161Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Compiling the model\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:31:51.931982Z","iopub.execute_input":"2022-08-21T12:31:51.932609Z","iopub.status.idle":"2022-08-21T12:31:51.943072Z","shell.execute_reply.started":"2022-08-21T12:31:51.932574Z","shell.execute_reply":"2022-08-21T12:31:51.941928Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Saving weights\nfilepath = \"model_weight_saved.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only = True, mode = 'min')\ndesired_callbacks = [checkpoint]","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:31:55.682598Z","iopub.execute_input":"2022-08-21T12:31:55.683675Z","iopub.status.idle":"2022-08-21T12:31:55.688813Z","shell.execute_reply.started":"2022-08-21T12:31:55.683630Z","shell.execute_reply":"2022-08-21T12:31:55.687733Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Fitting the model and Training\nmodel.fit(x,y, epochs = 47, batch_size = 256, callbacks = desired_callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:31:59.932038Z","iopub.execute_input":"2022-08-21T12:31:59.932731Z","iopub.status.idle":"2022-08-21T13:08:00.922026Z","shell.execute_reply.started":"2022-08-21T12:31:59.932697Z","shell.execute_reply":"2022-08-21T13:08:00.920848Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Epoch 1/47\n863/863 [==============================] - 49s 54ms/step - loss: 2.9235\n\nEpoch 00001: loss improved from inf to 2.92349, saving model to model_weight_saved.hdf5\nEpoch 2/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.9044\n\nEpoch 00002: loss improved from 2.92349 to 2.90443, saving model to model_weight_saved.hdf5\nEpoch 3/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.8777\n\nEpoch 00003: loss improved from 2.90443 to 2.87770, saving model to model_weight_saved.hdf5\nEpoch 4/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.8303\n\nEpoch 00004: loss improved from 2.87770 to 2.83030, saving model to model_weight_saved.hdf5\nEpoch 5/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.7661\n\nEpoch 00005: loss improved from 2.83030 to 2.76610, saving model to model_weight_saved.hdf5\nEpoch 6/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.6909\n\nEpoch 00006: loss improved from 2.76610 to 2.69086, saving model to model_weight_saved.hdf5\nEpoch 7/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.6314\n\nEpoch 00007: loss improved from 2.69086 to 2.63136, saving model to model_weight_saved.hdf5\nEpoch 8/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.5741\n\nEpoch 00008: loss improved from 2.63136 to 2.57415, saving model to model_weight_saved.hdf5\nEpoch 9/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.5250\n\nEpoch 00009: loss improved from 2.57415 to 2.52503, saving model to model_weight_saved.hdf5\nEpoch 10/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.4766\n\nEpoch 00010: loss improved from 2.52503 to 2.47657, saving model to model_weight_saved.hdf5\nEpoch 11/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.4305\n\nEpoch 00011: loss improved from 2.47657 to 2.43052, saving model to model_weight_saved.hdf5\nEpoch 12/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.3883\n\nEpoch 00012: loss improved from 2.43052 to 2.38825, saving model to model_weight_saved.hdf5\nEpoch 13/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.3516\n\nEpoch 00013: loss improved from 2.38825 to 2.35157, saving model to model_weight_saved.hdf5\nEpoch 14/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.3178\n\nEpoch 00014: loss improved from 2.35157 to 2.31777, saving model to model_weight_saved.hdf5\nEpoch 15/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.2867\n\nEpoch 00015: loss improved from 2.31777 to 2.28670, saving model to model_weight_saved.hdf5\nEpoch 16/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.2555\n\nEpoch 00016: loss improved from 2.28670 to 2.25551, saving model to model_weight_saved.hdf5\nEpoch 17/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.2293\n\nEpoch 00017: loss improved from 2.25551 to 2.22930, saving model to model_weight_saved.hdf5\nEpoch 18/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.2072\n\nEpoch 00018: loss improved from 2.22930 to 2.20717, saving model to model_weight_saved.hdf5\nEpoch 19/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.1833\n\nEpoch 00019: loss improved from 2.20717 to 2.18330, saving model to model_weight_saved.hdf5\nEpoch 20/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.1629\n\nEpoch 00020: loss improved from 2.18330 to 2.16293, saving model to model_weight_saved.hdf5\nEpoch 21/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.1447\n\nEpoch 00021: loss improved from 2.16293 to 2.14466, saving model to model_weight_saved.hdf5\nEpoch 22/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.1238\n\nEpoch 00022: loss improved from 2.14466 to 2.12384, saving model to model_weight_saved.hdf5\nEpoch 23/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.1083\n\nEpoch 00023: loss improved from 2.12384 to 2.10830, saving model to model_weight_saved.hdf5\nEpoch 24/47\n863/863 [==============================] - 46s 54ms/step - loss: 2.0932\n\nEpoch 00024: loss improved from 2.10830 to 2.09323, saving model to model_weight_saved.hdf5\nEpoch 25/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0778\n\nEpoch 00025: loss improved from 2.09323 to 2.07784, saving model to model_weight_saved.hdf5\nEpoch 26/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0636\n\nEpoch 00026: loss improved from 2.07784 to 2.06358, saving model to model_weight_saved.hdf5\nEpoch 27/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0480\n\nEpoch 00027: loss improved from 2.06358 to 2.04797, saving model to model_weight_saved.hdf5\nEpoch 28/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0362\n\nEpoch 00028: loss improved from 2.04797 to 2.03619, saving model to model_weight_saved.hdf5\nEpoch 29/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0231\n\nEpoch 00029: loss improved from 2.03619 to 2.02307, saving model to model_weight_saved.hdf5\nEpoch 30/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0115\n\nEpoch 00030: loss improved from 2.02307 to 2.01146, saving model to model_weight_saved.hdf5\nEpoch 31/47\n863/863 [==============================] - 46s 53ms/step - loss: 2.0000\n\nEpoch 00031: loss improved from 2.01146 to 2.00000, saving model to model_weight_saved.hdf5\nEpoch 32/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9862\n\nEpoch 00032: loss improved from 2.00000 to 1.98616, saving model to model_weight_saved.hdf5\nEpoch 33/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9782\n\nEpoch 00033: loss improved from 1.98616 to 1.97817, saving model to model_weight_saved.hdf5\nEpoch 34/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9698\n\nEpoch 00034: loss improved from 1.97817 to 1.96982, saving model to model_weight_saved.hdf5\nEpoch 35/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9593\n\nEpoch 00035: loss improved from 1.96982 to 1.95931, saving model to model_weight_saved.hdf5\nEpoch 36/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9507\n\nEpoch 00036: loss improved from 1.95931 to 1.95071, saving model to model_weight_saved.hdf5\nEpoch 37/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9446\n\nEpoch 00037: loss improved from 1.95071 to 1.94456, saving model to model_weight_saved.hdf5\nEpoch 38/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9355\n\nEpoch 00038: loss improved from 1.94456 to 1.93550, saving model to model_weight_saved.hdf5\nEpoch 39/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9265\n\nEpoch 00039: loss improved from 1.93550 to 1.92652, saving model to model_weight_saved.hdf5\nEpoch 40/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9178\n\nEpoch 00040: loss improved from 1.92652 to 1.91778, saving model to model_weight_saved.hdf5\nEpoch 41/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9086\n\nEpoch 00041: loss improved from 1.91778 to 1.90857, saving model to model_weight_saved.hdf5\nEpoch 42/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.9026\n\nEpoch 00042: loss improved from 1.90857 to 1.90256, saving model to model_weight_saved.hdf5\nEpoch 43/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.8968\n\nEpoch 00043: loss improved from 1.90256 to 1.89680, saving model to model_weight_saved.hdf5\nEpoch 44/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.8906\n\nEpoch 00044: loss improved from 1.89680 to 1.89056, saving model to model_weight_saved.hdf5\nEpoch 45/47\n863/863 [==============================] - 46s 53ms/step - loss: 1.8860\n\nEpoch 00045: loss improved from 1.89056 to 1.88595, saving model to model_weight_saved.hdf5\nEpoch 46/47\n863/863 [==============================] - 46s 54ms/step - loss: 1.8778\n\nEpoch 00046: loss improved from 1.88595 to 1.87781, saving model to model_weight_saved.hdf5\nEpoch 47/47\n863/863 [==============================] - 46s 54ms/step - loss: 1.8736\n\nEpoch 00047: loss improved from 1.87781 to 1.87356, saving model to model_weight_saved.hdf5\n","output_type":"stream"},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fedb05213d0>"},"metadata":{}}]},{"cell_type":"code","source":"# Recompile model with the saved weights\nfilename = 'model_weight_saved.hdf5'\nmodel.load_weights(filename)\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:08:11.225937Z","iopub.execute_input":"2022-08-21T13:08:11.226695Z","iopub.status.idle":"2022-08-21T13:08:11.253700Z","shell.execute_reply.started":"2022-08-21T13:08:11.226656Z","shell.execute_reply":"2022-08-21T13:08:11.252791Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# Output of the model back into characters\nnum_to_char = dict((i,c) for i,c in enumerate(chars))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:08:14.652511Z","iopub.execute_input":"2022-08-21T13:08:14.653150Z","iopub.status.idle":"2022-08-21T13:08:14.658393Z","shell.execute_reply.started":"2022-08-21T13:08:14.653104Z","shell.execute_reply":"2022-08-21T13:08:14.657248Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Random seed to help generate\nstart = np.random.randint(0, (len(x_data) - 1))\npattern = x_data[start]\nprint('Random Seed: ')\nprint(\"\\\"\",''.join([num_to_char[value] for value in pattern]),\"\", \"\\\"\")","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:09:50.622689Z","iopub.execute_input":"2022-08-21T13:09:50.623156Z","iopub.status.idle":"2022-08-21T13:09:50.631414Z","shell.execute_reply.started":"2022-08-21T13:09:50.623124Z","shell.execute_reply":"2022-08-21T13:09:50.630229Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Random Seed: \n\" inquishedpublicfunctionsimmediatelyunionsoughtpleasantclimateitalychangesceneinterestattendanttourla  \"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate the text\nfor i in range(1000):\n    x = np.reshape(pattern, (1,len(pattern), 1))\n    x = x/float(vocab_len)\n    prediction = model.predict(x, verbose = 0)\n    index = np.argmax(prediction)\n    result = num_to_char[index]\n    seq_in = [num_to_char[value] for value in pattern]\n    sys.stdout.write(result)\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:10:21.683551Z","iopub.execute_input":"2022-08-21T13:10:21.684552Z","iopub.status.idle":"2022-08-21T13:11:05.124065Z","shell.execute_reply.started":"2022-08-21T13:10:21.684515Z","shell.execute_reply":"2022-08-21T13:11:05.122726Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"besenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssenderedsearonsendeavouredsearonsendersaidconsiderableseveralhourssendered","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}